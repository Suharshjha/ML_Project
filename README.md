# ğŸ“˜ Visual Question Answering (VQA) using BLIP-VQA
Overcoming VL-T5 Implementation Challenges in a Lab Environment


# ğŸ“Œ Overview
- This project implements Visual Question Answering (VQA) using BLIP-VQA, a fully pretrained Vision-Language model by Salesforce.
- The system answers text questions about an uploaded image â€” image in â†’ answer out.
- Our initial goal was to reproduce the research model VL-T5, but due to practical limitations, we switched to BLIP-VQA which works smoothly on a standard laptop while still giving accurate results.

# ğŸ§  Why VL-T5 Failed
- âŒ No pretrained multimodal weights available
- âŒ Needed exactly 36 Faster R-CNN region features per image
- âŒ Detectron2 installation fails on Windows
- âŒ Region count mismatch â†’ tensor shape errors
- âŒ Without pretrained weights, output became random
- âŒ Required huge datasets (COCO, Visual Genome, VQA)
- âŒ Required A100-level GPUs and weeks of training

ğŸ”¹â­ Conclusion: VL-T5 is not feasible in a typical academic lab environment.

# ğŸ’¡ Why BLIP-VQA Was the Perfect Solution
- âœ” No region feature extraction required
- âœ” No Detectron2 installation
- âœ” Works on a normal laptop CPU
- âœ” Produces correct, meaningful answers
- âœ” Very easy to implement (end-to-end)

BLIP Pipeline:
```bash
Image â†’ Vision Transformer â†’ Text Answer
```

# ğŸš€ Features
- âœ” Upload an image and ask any question about it
- âœ” Generates meaningful answers like: â€œBecause it is raining.â€,â€œSitting on a suitcase.â€
- âœ” Fast inference
- âœ” No GPU required
- âœ” Zero complex setup
- âœ” Fully reproducible for ML lab projects

# ğŸ“‚ Project Structure
```bash
â”œâ”€â”€ app.py                     # Main interface (Streamlit/Flask/Gradio)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ blip_vqa.py            # BLIP-VQA model loading & inference
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ image_utils.py         # Preprocessing helpers
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

# ğŸ”§ Installation
```bash
git clone <your-repo-url>
cd <project-folder>

pip install -r requirements.txt
```

# ğŸ Final Conclusion
- VL-T5 is theoretically strong but practically impossible to reproduce in a normal lab due to missing pretrained weights, strict region requirements, and heavy GPU needs.
- By switching to BLIP-VQA, we achieved a:
    âœ” Fully functional
    âœ” Lab-friendly
    âœ” High-accuracy
    âœ” End-to-end VQA system
- that runs on a standard laptop with minimal dependencies.



